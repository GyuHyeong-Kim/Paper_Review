{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a0c34d-8f44-465c-b0aa-7707bbecfab0",
   "metadata": {},
   "source": [
    "# ResNet18_CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a4b7cd-bf20-4f01-9e72-20b0cd5386ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498b52f-83bb-4371-a987-3761a3e4eaad",
   "metadata": {},
   "source": [
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e44c651-bc2b-464d-950f-5885ac6f1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride = 1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size = 3, stride = stride, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential() # identity\n",
    "\n",
    "        if stride != 1: # projection\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size = 1, stride = stride, bias = False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x) # skip connection, shortcut connection\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e19fa01-69f8-490d-90b4-2976a0189973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride = 1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride = 2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride = 2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out - self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc769860-39fb-460e-955d-2294bc885db8",
   "metadata": {},
   "source": [
    "### DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98d6c20b-ec17-4bfc-ba7f-2863a37ee7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding = 4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True, transform = transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root = './data', train = False, download = True, transform = transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 128, shuffle = True, num_workers = 4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df10a7d-0d43-488d-a6d1-ceee9cf2ad40",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e96b2bf-7ad6-4b5f-9150-e0cbc37c289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "net = torch.nn.DataParallel(net)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet18_cifar10.pt'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum = 0.9, weight_decay = 0.0002)\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\n[ Train epoch: %d ]' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        benign_outputs = net(inputs)\n",
    "        loss = criterion(benign_outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = benign_outputs.max(1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\nCurrent batch:', str(batch_idx))\n",
    "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
    "            print('Current benign train loss:', loss.item())\n",
    "\n",
    "    print('\\nTotal benign train accuarcy:', 100. * correct / total)\n",
    "    print('Total benign train loss:', train_loss)\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    print('\\n[ Test epoch: %d ]' % epoch)\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        total += targets.size(0)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss += criterion(outputs, targets).item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print('\\nTest accuarcy:', 100. * correct / total)\n",
    "    print('Test average loss:', loss / total)\n",
    "\n",
    "    state = {'net': net.state_dict()}\n",
    "    \n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/' + file_name)\n",
    "    print('Model Saved!')\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    if epoch >= 150:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16fe2d00-7dd8-480c-a908-f7cad8ce71a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Train epoch: 0 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.0859375\n",
      "Current benign train loss: 2.4893953800201416\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.203125\n",
      "Current benign train loss: 2.098731279373169\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.3515625\n",
      "Current benign train loss: 1.7149142026901245\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.4140625\n",
      "Current benign train loss: 1.592165470123291\n",
      "\n",
      "Total benign train accuarcy: 31.746\n",
      "Total benign train loss: 778.0032021999359\n",
      "\n",
      "[ Test epoch: 0 ]\n",
      "\n",
      "Test accuarcy: 34.68\n",
      "Test average loss: 0.01980307376384735\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 1 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.4140625\n",
      "Current benign train loss: 1.5717706680297852\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.390625\n",
      "Current benign train loss: 1.6239955425262451\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.4375\n",
      "Current benign train loss: 1.4547293186187744\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.5234375\n",
      "Current benign train loss: 1.3255184888839722\n",
      "\n",
      "Total benign train accuarcy: 47.42\n",
      "Total benign train loss: 562.6591440439224\n",
      "\n",
      "[ Test epoch: 1 ]\n",
      "\n",
      "Test accuarcy: 46.35\n",
      "Test average loss: 0.014759357178211212\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 2 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.515625\n",
      "Current benign train loss: 1.2320619821548462\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.53125\n",
      "Current benign train loss: 1.2373124361038208\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.546875\n",
      "Current benign train loss: 1.044597864151001\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.609375\n",
      "Current benign train loss: 1.1857761144638062\n",
      "\n",
      "Total benign train accuarcy: 56.406\n",
      "Total benign train loss: 471.2506195306778\n",
      "\n",
      "[ Test epoch: 2 ]\n",
      "\n",
      "Test accuarcy: 62.42\n",
      "Test average loss: 0.010394073796272277\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 3 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.59375\n",
      "Current benign train loss: 1.0511707067489624\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.609375\n",
      "Current benign train loss: 1.0958458185195923\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.6953125\n",
      "Current benign train loss: 0.8601744174957275\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.625\n",
      "Current benign train loss: 1.073664665222168\n",
      "\n",
      "Total benign train accuarcy: 64.03\n",
      "Total benign train loss: 394.18706423044205\n",
      "\n",
      "[ Test epoch: 3 ]\n",
      "\n",
      "Test accuarcy: 61.36\n",
      "Test average loss: 0.011272185724973679\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 4 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 0.8892706632614136\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.671875\n",
      "Current benign train loss: 1.0279271602630615\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.6875\n",
      "Current benign train loss: 0.9633074402809143\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.625\n",
      "Current benign train loss: 1.0159101486206055\n",
      "\n",
      "Total benign train accuarcy: 69.322\n",
      "Total benign train loss: 342.4874838590622\n",
      "\n",
      "[ Test epoch: 4 ]\n",
      "\n",
      "Test accuarcy: 63.69\n",
      "Test average loss: 0.010674732267856597\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 5 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.765625\n",
      "Current benign train loss: 0.6983056664466858\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.734375\n",
      "Current benign train loss: 0.7140787839889526\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.703125\n",
      "Current benign train loss: 0.847179651260376\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 0.7305114269256592\n",
      "\n",
      "Total benign train accuarcy: 73.212\n",
      "Total benign train loss: 299.14227414131165\n",
      "\n",
      "[ Test epoch: 5 ]\n",
      "\n",
      "Test accuarcy: 67.04\n",
      "Test average loss: 0.00946259235739708\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 6 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.65625\n",
      "Current benign train loss: 0.9529685974121094\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.71875\n",
      "Current benign train loss: 0.8425325751304626\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.765625\n",
      "Current benign train loss: 0.7210735082626343\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.5258848667144775\n",
      "\n",
      "Total benign train accuarcy: 76.632\n",
      "Total benign train loss: 261.9469206929207\n",
      "\n",
      "[ Test epoch: 6 ]\n",
      "\n",
      "Test accuarcy: 64.53\n",
      "Test average loss: 0.011313013881444931\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 7 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.796875\n",
      "Current benign train loss: 0.611524760723114\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.796875\n",
      "Current benign train loss: 0.5605670809745789\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.5615738034248352\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.7734375\n",
      "Current benign train loss: 0.6546499133110046\n",
      "\n",
      "Total benign train accuarcy: 79.02\n",
      "Total benign train loss: 235.0250964164734\n",
      "\n",
      "[ Test epoch: 7 ]\n",
      "\n",
      "Test accuarcy: 68.5\n",
      "Test average loss: 0.009986635905504226\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 8 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.796875\n",
      "Current benign train loss: 0.649329662322998\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 0.5948054194450378\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.4396185278892517\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.4668560326099396\n",
      "\n",
      "Total benign train accuarcy: 80.86\n",
      "Total benign train loss: 215.62689593434334\n",
      "\n",
      "[ Test epoch: 8 ]\n",
      "\n",
      "Test accuarcy: 73.01\n",
      "Test average loss: 0.008163120192289352\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 9 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.5201554298400879\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.875\n",
      "Current benign train loss: 0.36308667063713074\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.828125\n",
      "Current benign train loss: 0.5101000666618347\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 0.6147003173828125\n",
      "\n",
      "Total benign train accuarcy: 82.594\n",
      "Total benign train loss: 197.01987886428833\n",
      "\n",
      "[ Test epoch: 9 ]\n",
      "\n",
      "Test accuarcy: 72.1\n",
      "Test average loss: 0.008374307990074157\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 10 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.45716506242752075\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.519476592540741\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8125\n",
      "Current benign train loss: 0.49737584590911865\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8125\n",
      "Current benign train loss: 0.5017011165618896\n",
      "\n",
      "Total benign train accuarcy: 83.864\n",
      "Total benign train loss: 181.4287042915821\n",
      "\n",
      "[ Test epoch: 10 ]\n",
      "\n",
      "Test accuarcy: 80.62\n",
      "Test average loss: 0.005764443174004555\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 11 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.31748563051223755\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.530262291431427\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.47680988907814026\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.875\n",
      "Current benign train loss: 0.32358574867248535\n",
      "\n",
      "Total benign train accuarcy: 84.746\n",
      "Total benign train loss: 172.9500852227211\n",
      "\n",
      "[ Test epoch: 11 ]\n",
      "\n",
      "Test accuarcy: 78.7\n",
      "Test average loss: 0.006138390290737152\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 12 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.4240136444568634\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8125\n",
      "Current benign train loss: 0.5911489725112915\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.46165117621421814\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.3368666172027588\n",
      "\n",
      "Total benign train accuarcy: 85.486\n",
      "Total benign train loss: 162.02475576102734\n",
      "\n",
      "[ Test epoch: 12 ]\n",
      "\n",
      "Test accuarcy: 77.33\n",
      "Test average loss: 0.007395925423502922\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 13 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.4470149278640747\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.40787962079048157\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8828125\n",
      "Current benign train loss: 0.35252800583839417\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.38556957244873047\n",
      "\n",
      "Total benign train accuarcy: 86.21\n",
      "Total benign train loss: 156.29297423362732\n",
      "\n",
      "[ Test epoch: 13 ]\n",
      "\n",
      "Test accuarcy: 83.72\n",
      "Test average loss: 0.004849410676956177\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 14 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.38346874713897705\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.3987375795841217\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.3635251224040985\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.3203800916671753\n",
      "\n",
      "Total benign train accuarcy: 87.028\n",
      "Total benign train loss: 148.19097888469696\n",
      "\n",
      "[ Test epoch: 14 ]\n",
      "\n",
      "Test accuarcy: 81.65\n",
      "Test average loss: 0.005694676688313484\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 15 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.875\n",
      "Current benign train loss: 0.31876084208488464\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.3174828588962555\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8828125\n",
      "Current benign train loss: 0.311990886926651\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.3230268657207489\n",
      "\n",
      "Total benign train accuarcy: 87.478\n",
      "Total benign train loss: 142.70581184327602\n",
      "\n",
      "[ Test epoch: 15 ]\n",
      "\n",
      "Test accuarcy: 79.02\n",
      "Test average loss: 0.0062208904445171355\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 16 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.43576470017433167\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.34972724318504333\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.28712329268455505\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.875\n",
      "Current benign train loss: 0.3296813666820526\n",
      "\n",
      "Total benign train accuarcy: 87.8\n",
      "Total benign train loss: 137.62242703139782\n",
      "\n",
      "[ Test epoch: 16 ]\n",
      "\n",
      "Test accuarcy: 82.65\n",
      "Test average loss: 0.005588816855847835\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 17 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.3112734854221344\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.2700868844985962\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.341421902179718\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.9375\n",
      "Current benign train loss: 0.22788037359714508\n",
      "\n",
      "Total benign train accuarcy: 88.332\n",
      "Total benign train loss: 131.91022822260857\n",
      "\n",
      "[ Test epoch: 17 ]\n",
      "\n",
      "Test accuarcy: 81.53\n",
      "Test average loss: 0.005644294735789299\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 18 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.32619452476501465\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.9140625\n",
      "Current benign train loss: 0.26479652523994446\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.921875\n",
      "Current benign train loss: 0.2669340670108795\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.294097900390625\n",
      "\n",
      "Total benign train accuarcy: 88.896\n",
      "Total benign train loss: 126.34587082266808\n",
      "\n",
      "[ Test epoch: 18 ]\n",
      "\n",
      "Test accuarcy: 83.7\n",
      "Test average loss: 0.005048184984922409\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 19 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8828125\n",
      "Current benign train loss: 0.24961213767528534\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.28584587574005127\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.2597705125808716\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.287026971578598\n",
      "\n",
      "Total benign train accuarcy: 89.046\n",
      "Total benign train loss: 123.7832633703947\n",
      "\n",
      "[ Test epoch: 19 ]\n",
      "\n",
      "Test accuarcy: 85.78\n",
      "Test average loss: 0.00431037253588438\n",
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 20):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
